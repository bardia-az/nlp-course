{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neuralmt: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:04,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was in my first time . \n",
      "i was in and i . \n",
      "she was a woman . \n",
      "so she was to , and she was , and she was , and she was , and she was , and she was in the . \n",
      "i was i . \n",
      "a 's a first . \n",
      "i got to talk about the woman . \n",
      "i i i . \n",
      "i did n't . \n",
      "so , the the , the the , , the the , , the the , , the the , , \n",
      "i was , , \" i , \" i said , \" \n",
      "later , later later later later later later later later later later later . \n",
      "i was in the . \n",
      "i asked to to , i was . \n",
      "i . \n",
      "she 's , , \" she , , \" she 's , , \" she . \n",
      "but , you know , but you . \n",
      "the , the is is that it 's is , \" \n",
      "what 's what is . \n",
      "that was the moment that was that the moment . \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_path = 'data/input/dev.txt'\n",
    "num = 20\n",
    "model = Seq2Seq(build=False)\n",
    "model.load(os.path.join('data', 'seq2seq_E049.pt'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# loading test dataset\n",
    "test_dl = loadTestData(input_path, model.params['srcLex'],\n",
    "                       device=hp.device, linesToLoad= num)\n",
    "results = translate(model, test_dl)\n",
    "print(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 4.51 54.9/16.7/5.6/1.6 (BP = 0.471 ratio = 0.571 hyp_len = 182 ref_len = 319)\n"
     ]
    }
   ],
   "source": [
    "from bleu_check import bleu\n",
    "ref_t = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    ref_t = r.read().strip().splitlines()\n",
    "print(bleu(ref_t, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "### The Attention Layer:\n",
    "\n",
    "The introduction of the attention layer within autoencoders allows the model to learn which parts of the input data are most relevant for a given context, effectively enabling the model to focus its reconstruction efforts on those elements. The attention mechanism in our model, as provided in the AttentionModule class, comprises key components:\n",
    "\n",
    "The Weighting Mechanism: The attention mechanism calculates a set of attention weights (alpha) for each element in the input sequence. These weights are learned through the W_enc and W_dec linear transformations and the V_att linear layer, making them adapt to the specific relationships between the encoder's output and the decoder's hidden state.\n",
    "\n",
    "The Context Calculation: Once the attention weights are calculated, they are used to compute a context vector, which summarizes the most relevant information from the input sequence. In the provided implementation, the context is computed efficiently using a batched matrix multiplication, which significantly improves computational performance.\n",
    "\n",
    "As was mentioned in the documentation, attention is defined as follows:\n",
    "\\[\\mathrm{score}_i = W_{enc}( h^{enc}_i ) + W_{dec}( h^{dec} )\\]\n",
    "Define the $\\alpha$ vector as follows:\n",
    "\n",
    "\\[\\alpha = \\mathrm{softmax}(V_{att} \\mathrm{tanh} (\\mathrm{score}))\\]\n",
    "The we define the context vector using the $\\alpha$ weights for each source side index $i$:\n",
    "\n",
    "\\[c = \\sum_i \\alpha_i \\times h^{enc}_i\\]\n",
    "\n",
    "### Effect of Attention Mechanism:\n",
    "\n",
    "The introduction of the attention mechanism into the autoencoder model has a profound impact on its performance. This can be observed through the significant increase in the BLEU score from 1.8637 to 14.2469, indicating the substantial improvement in the model's ability to capture and reconstruct input sequences.\n",
    "\n",
    "The attention mechanism allows the model to focus on the most critical elements of the input data during the decoding process, resulting in improved reconstruction quality. This is especially beneficial for tasks where certain elements in the input sequence are more important than others, such as machine translation or text summarization.\n",
    "\n",
    "#### here is the implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, attention_dim):\n",
    "        \"\"\"\n",
    "        You shouldn't deleted/change any of the following defs, they are\n",
    "        essential for successfully loading the saved model.\n",
    "        \"\"\"\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.W_enc = nn.Linear(attention_dim, attention_dim, bias=False)\n",
    "        self.W_dec = nn.Linear(attention_dim, attention_dim, bias=False)\n",
    "        self.V_att = nn.Linear(attention_dim, 1, bias=False)\n",
    "        self.softmax = nn.Softmax(dim = 0)\n",
    "\n",
    "    # Start working from here, both 'calcAlpha' and 'forward' need to be fixed\n",
    "    def calcAlpha(self, decoder_hidden, encoder_out):\n",
    "        \"\"\"\n",
    "        param encoder_out: (seq, batch, dim),\n",
    "        param decoder_hidden: (seq, batch, dim)\n",
    "        \"\"\"\n",
    "        enc = self.W_enc( encoder_out )\n",
    "        dec = self.W_dec( decoder_hidden )\n",
    "        scores = enc + dec\n",
    "        beta = self.V_att( torch.nn.functional.tanh( scores ) )\n",
    "        alpha = self.softmax( beta )\n",
    "        return alpha\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_out):\n",
    "        \"\"\"\n",
    "        encoder_out: (seq, batch, dim),\n",
    "        decoder_hidden: (seq, batch, dim)\n",
    "        \"\"\"\n",
    "        alpha = self.calcAlpha(decoder_hidden, encoder_out) # seq, batch, dim=1\n",
    "        context = torch.sum(alpha * encoder_out, dim=0).unsqueeze(0)\n",
    "        return context, alpha.permute(2, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## beam search implementation\n",
    "\n",
    "\n",
    "The provided beam search implementation efficiently decodes the output sequence from a sequence-to-sequence model while incorporating beam search to improve search performance. The function takes the decoder model, encoder output, encoder hidden state, maximum sequence length, beam width, and an optional maximum iteration limit as input parameters.\n",
    "\n",
    "### Initialization:\n",
    "\n",
    "    Initialize variables to store output sequences, target vocabulary size, decoder hidden state, and start token.\n",
    "    Create a priority queue nodes_cach to maintain beam candidates.\n",
    "    Initialize an empty list end_nodes to store finalized sequences.\n",
    "\n",
    "### Decoding Steps:\n",
    "\n",
    "    Start with the start token and initial decoder hidden state.\n",
    "    Calculate the log-probability and token with the highest probability using softmax.\n",
    "    Create an initial beam node with the token, score, and decoder hidden state.\n",
    "    Push the initial node onto the priority queue.\n",
    "\n",
    "### Beam Search Loop:\n",
    "\n",
    "    While the number of finalized sequences is less than the beam width:\n",
    "    Check if the maximum iteration limit is exceeded. If so, finalize all remaining beam candidates.\n",
    "    Pop the beam node with the highest score from the priority queue.\n",
    "    Update the current token and decoder hidden state based on the popped node.\n",
    "    Calculate the top-K most probable tokens and their corresponding log-probabilities.\n",
    "    Iterate through the top-K tokens:\n",
    "    Create a new beam node for each token with updated score, previous node, length, logits, and decoder hidden state.( as suggested in n (Bahdanau et al., 2014), to break beams search curse, normalizing the score by the sequence length helps us achive a better blue score since it is observed that higher beam width results in smaller BLEU score)\n",
    "    If the new node's length reaches the maximum sequence length, add it to the finalized sequences list.\n",
    "    Otherwise, push the new node onto the priority queue.\n",
    "    Increment the iteration counter.\n",
    "    \n",
    "### Finalization and Output:\n",
    "\n",
    "    Sort the finalized sequences based on their scores.\n",
    "    Select the best sequence with the highest score.\n",
    "    Reconstruct the output sequence and target sequences using the best node's get_seq() method.\n",
    "    Return the output sequence, attention weights (None in this case), and target sequences.\n",
    "    \n",
    "This implementation effectively utilizes beam search to explore multiple promising paths during decoding, leading to improved sequence generation compared to greedy search. The maximum iteration limit ensures that the decoding process doesn't get stuck in an endless loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addition of an attention layer to the model improved the BLEU score significantly, indicating that the model's ability to focus on relevant parts of the input sequence during decoding enhanced its translation quality. We also tried Ensembling and Beam Searching:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling\n",
    "We load all the check point files existing in the directory that --model command line option point to, and create a list of Seq2Seq models. Then we run the translation on all the models and obtain the scores of each token in the destination vocabulary for the current word. First we tried to add those scores for all the instances of Seq2Seq models and in the end, choose the maximum value. However, it worsened the results. Then, we decided to choose the maximum-score (best) token for each of the models, and then select the token that has been chosen with most of the models. In this case, the results were better but still worse than the baseline. So, we couldn't succeed in Ensembling.\n",
    "Note that, the --model command line option now can point to a checkpoint file or a directory. In the first case, only the selected checkpoint file is loaded and of course no ensembling happens. In the second case, the code loads all the checkpoint files available in the directory and runs ensembling inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Searching\n",
    "However, the beam search algorithm did not have a noticeable impact on the results even width higher beam width results remained the same. This could be due to several factors:\n",
    "\n",
    "\n",
    "#### Model Complexity:\n",
    "    The increased complexity of the model with the attention layer might have made it more difficult for the beam search algorithm to effectively prune less promising paths. The beam search might not have been able to differentiate between equally probable translations, leading to similar outputs.\n",
    "\n",
    "#### Data Quality:\n",
    "    The quality of the training data might have played a role in the beam search's performance. If the training data did not contain enough examples of diverse and complex translations, the beam search might not have been able to learn effective strategies for distinguishing between different translation candidates.\n",
    "\n",
    "#### Hyperparameter Tuning: \n",
    "    The hyperparameters of the beam search algorithm, such as the beam width and maximum iteration limit, might not have been optimally tuned for the specific model and dataset. Fine-tuning these hyperparameters could potentially improve the performance of the beam search.\n",
    "\n",
    "#### Attention Mechanism:\n",
    "    The specific attention mechanism used in the model might not have been well-suited for the task or the beam search algorithm. Different attention mechanisms can have varying effects on the model's ability to focus on relevant parts of the input sequence and guide the beam search process.\n",
    "\n",
    "In conclusion, while the attention layer significantly improved the model's translation quality, the beam search algorithm did not have a noticeable impact in this particular case. Further investigation into the factors mentioned above could help identify ways to optimize the beam search algorithm and enhance its effectiveness in conjunction with the attention layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
