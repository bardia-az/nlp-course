{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neuralmt: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bardia_az/nlp/nlpclass-1237-g-savagetokenizer/hw3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bardia_az/nlp/nlpclass-1237-g-savagetokenizer/hw3/venv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:04,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was in my first time . \n",
      "i was in and i . \n",
      "she was a woman . \n",
      "so she was to , and she was , and she was , and she was , and she was , and she was in the . \n",
      "i was i . \n",
      "a 's a first . \n",
      "i got to talk about the woman . \n",
      "i i i . \n",
      "i did n't . \n",
      "so , the the , the the , , the the , , the the , , the the , , \n",
      "i was , , \" i , \" i said , \" \n",
      "later , later later later later later later later later later later later . \n",
      "i was in the . \n",
      "i asked to to , i was . \n",
      "i . \n",
      "she 's , , \" she , , \" she 's , , \" she . \n",
      "but , you know , but you . \n",
      "the , the is is that it 's is , \" \n",
      "what 's what is . \n",
      "that was the moment that was that the moment . \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_path = 'data/input/dev.txt'\n",
    "num = 20\n",
    "model = Seq2Seq(build=False)\n",
    "model.load(os.path.join('data', 'seq2seq_E049.pt'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# loading test dataset\n",
    "test_dl = loadTestData(input_path, model.params['srcLex'],\n",
    "                       device=hp.device, linesToLoad= num)\n",
    "results = translate(model, test_dl)\n",
    "print(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 4.51 54.9/16.7/5.6/1.6 (BP = 0.471 ratio = 0.571 hyp_len = 182 ref_len = 319)\n"
     ]
    }
   ],
   "source": [
    "from bleu_check import bleu\n",
    "ref_t = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    ref_t = r.read().strip().splitlines()\n",
    "print(bleu(ref_t, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "### The Attention Layer:\n",
    "\n",
    "The introduction of the attention layer within autoencoders allows the model to learn which parts of the input data are most relevant for a given context, effectively enabling the model to focus its reconstruction efforts on those elements. The attention mechanism in our model, as provided in the AttentionModule class, comprises key components:\n",
    "\n",
    "The Weighting Mechanism: The attention mechanism calculates a set of attention weights (alpha) for each element in the input sequence. These weights are learned through the W_enc and W_dec linear transformations and the V_att linear layer, making them adapt to the specific relationships between the encoder's output and the decoder's hidden state.\n",
    "\n",
    "The Context Calculation: Once the attention weights are calculated, they are used to compute a context vector, which summarizes the most relevant information from the input sequence. In the provided implementation, the context is computed efficiently using a batched matrix multiplication, which significantly improves computational performance.\n",
    "\n",
    "As was mentioned in the documentation, attention is defined as follows:\n",
    "\\[\\mathrm{score}_i = W_{enc}( h^{enc}_i ) + W_{dec}( h^{dec} )\\]\n",
    "Define the $\\alpha$ vector as follows:\n",
    "\n",
    "\\[\\alpha = \\mathrm{softmax}(V_{att} \\mathrm{tanh} (\\mathrm{score}))\\]\n",
    "The we define the context vector using the $\\alpha$ weights for each source side index $i$:\n",
    "\n",
    "\\[c = \\sum_i \\alpha_i \\times h^{enc}_i\\]\n",
    "\n",
    "### Effect of Attention Mechanism:\n",
    "\n",
    "The introduction of the attention mechanism into the autoencoder model has a profound impact on its performance. This can be observed through the significant increase in the BLEU score from 1.8637 to 14.2469, indicating the substantial improvement in the model's ability to capture and reconstruct input sequences.\n",
    "\n",
    "The attention mechanism allows the model to focus on the most critical elements of the input data during the decoding process, resulting in improved reconstruction quality. This is especially beneficial for tasks where certain elements in the input sequence are more important than others, such as machine translation or text summarization.\n",
    "\n",
    "#### here is the implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, attention_dim):\n",
    "        \"\"\"\n",
    "        You shouldn't deleted/change any of the following defs, they are\n",
    "        essential for successfully loading the saved model.\n",
    "        \"\"\"\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.W_enc = nn.Linear(attention_dim, attention_dim, bias=False)\n",
    "        self.W_dec = nn.Linear(attention_dim, attention_dim, bias=False)\n",
    "        self.V_att = nn.Linear(attention_dim, 1, bias=False)\n",
    "        self.softmax = nn.Softmax(dim = 0)\n",
    "\n",
    "    # Start working from here, both 'calcAlpha' and 'forward' need to be fixed\n",
    "    def calcAlpha(self, decoder_hidden, encoder_out):\n",
    "        \"\"\"\n",
    "        param encoder_out: (seq, batch, dim),\n",
    "        param decoder_hidden: (seq, batch, dim)\n",
    "        \"\"\"\n",
    "        enc = self.W_enc( encoder_out )\n",
    "        dec = self.W_dec( decoder_hidden )\n",
    "        scores = enc + dec\n",
    "        beta = self.V_att( torch.nn.functional.tanh( scores ) )\n",
    "        alpha = self.softmax( beta )\n",
    "        return alpha\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_out):\n",
    "        \"\"\"\n",
    "        encoder_out: (seq, batch, dim),\n",
    "        decoder_hidden: (seq, batch, dim)\n",
    "        \"\"\"\n",
    "        alpha = self.calcAlpha(decoder_hidden, encoder_out) # seq, batch, dim=1\n",
    "        context = torch.sum(alpha * encoder_out, dim=0).unsqueeze(0)\n",
    "        return context, alpha.permute(2, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried Ensembling and Beam Searching:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling\n",
    "We load all the check point files existing in the directory that --model command line option point to, and create a list of Seq2Seq models. Then we run the translation on all the models and obtain the scores of each token in the destination vocabulary for the current word. First we tried to add those scores for all the instances of Seq2Seq models and in the end, choose the maximum value. However, it worsened the results. Then, we decided to choose the maximum-score (best) token for each of the models, and then select the token that has been chosen with most of the models. In this case, the results were better but still worse than the baseline. So, we couldn't succeed in Ensembling.\n",
    "Note that, the --model command line option now can point to a checkpoint file or a directory. In the first case, only the selected checkpoint file is loaded and of course no ensembling happens. In the second case, the code loads all the checkpoint files available in the directory and runs ensembling inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Searching"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
