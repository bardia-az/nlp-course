{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bertchunker: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertchunker import *\n",
    "import os, sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1027/1027 [00:20<00:00, 51.11it/s]\n"
     ]
    }
   ],
   "source": [
    "chunker = FinetuneTagger(os.path.join('..', 'data', 'chunker'), modelsuffix='.pt')\n",
    "decoder_output = chunker.decode(os.path.join('..', 'data', 'input', 'dev.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the warnings from the transformers library. They are expected to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processed 23663 tokens with 11896 phrases; found: 13226 phrases; correct: 9689.\n",
      "accuracy:  87.04%; (non-O)\n",
      "accuracy:  87.45%; precision:  73.26%; recall:  81.45%; FB1:  77.14\n",
      "             ADJP: precision:  13.32%; recall:  53.98%; FB1:  21.37  916\n",
      "             ADVP: precision:  31.16%; recall:  58.79%; FB1:  40.73  751\n",
      "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  8\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  11\n",
      "              LST: precision:   0.00%; recall:   0.00%; FB1:   0.00  3\n",
      "               NP: precision:  80.58%; recall:  80.86%; FB1:  80.72  6258\n",
      "               PP: precision:  95.97%; recall:  86.93%; FB1:  91.23  2211\n",
      "              PRT: precision:  22.15%; recall:  77.78%; FB1:  34.48  158\n",
      "             SBAR: precision:  36.12%; recall:  80.17%; FB1:  49.80  526\n",
      "              UCP: precision:   0.00%; recall:   0.00%; FB1:   0.00  64\n",
      "               VP: precision:  83.75%; recall:  84.33%; FB1:  84.04  2320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(73.25722062603963, 81.44754539340954, 77.13557837751772)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_output = [ output for sent in decoder_output for output in sent ]\n",
    "sys.path.append('..')\n",
    "import conlleval\n",
    "true_seqs = []\n",
    "with open(os.path.join('..', 'data', 'reference', 'dev.out')) as r:\n",
    "    for sent in conlleval.read_file(r):\n",
    "        true_seqs += sent.split()\n",
    "conlleval.evaluate(true_seqs, flat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "In order to improve the performance of the the fine-tuned model we tried a couple of ideas explained in the following:\n",
    "\n",
    "### 1) Data augmentation\n",
    "in order to apply data augmentation, we need to select what type of noise should we use in order to enhance the performance of the model and achive better results.\n",
    "looking at dev.txt data, it seems like added noise consists of changing or deleting or replacing characters in some tokens.\n",
    "\n",
    "We implemented all these types of manipulation mentioned in https://www.aclweb.org/anthology/P19-1561/ as well, including \"swap\", \"drop\", \"add\", and \"replace\".\n",
    "\n",
    "in order to   achive this goal, following function is implemented , which takes sentence and augmentation percentage as an input and applies data augmentation on the sequence before passing it to the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sentence( sent, aug_perc ):\n",
    "    chance = torch.rand( len(sent) )\n",
    "    selected_tokens = torch.where( chance <  aug_perc)[0]\n",
    "    sent = list( sent )\n",
    "    augmented = False\n",
    "    \n",
    "    for ind in selected_tokens:\n",
    "        token = sent[ind]   \n",
    "        if len( token ) < 3 :\n",
    "            continue\n",
    "        as_list = list( token )\n",
    "        operation = random.choice([\"swap\", \"insert\", \"drop\", \"replace\"])\n",
    "        \n",
    "        if operation == \"replace\":\n",
    "            selected_ind = random.randint(0, len(token) -1)\n",
    "            if as_list[selected_ind] not in aug_tokens:\n",
    "                continue\n",
    "            new_char = aug_tokens[random.randint(0, len(aug_tokens)-1)]\n",
    "            as_list[selected_ind] = new_char\n",
    "        elif operation == \"drop\":\n",
    "            selected_ind = random.randint(0, len(token) -1)\n",
    "            if as_list[selected_ind] not in aug_tokens:\n",
    "                continue\n",
    "            del as_list[selected_ind]\n",
    "        elif operation == \"swap\":\n",
    "            selected_ind = random.randint(0, len(token) -2)\n",
    "            as_list[selected_ind], as_list[selected_ind+1] = as_list[selected_ind+1], as_list[selected_ind]\n",
    "        elif operation == \"insert\":\n",
    "            selected_ind = random.randint(0, len(token))\n",
    "            new_char = aug_tokens[random.randint(0, len(aug_tokens)-1)]\n",
    "            as_list.insert(selected_ind, new_char)\n",
    "        \n",
    "        sent[ind] = \"\".join(as_list)\n",
    "        augmented = True\n",
    "    return tuple(sent), augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Two optimizers and learning rate schedulers\n",
    "Since in most cases, MLPs work better with SGD optimizer, a second optimzer for classification_head is applied to the model with the initial lr = .1\n",
    "We also utilized ExponentialLR schedulers for both optimizers to decay the learning rate throughout the training process.\n",
    "\n",
    "### 3) Improving classification head\n",
    "\n",
    "in order to improve classification head, single linear layer is replaced with the following sequential model:\n",
    "\n",
    "  (classification_head): Sequential(\n",
    "\n",
    "    (0): Dropout(p=0.2, inplace=False)\n",
    "    \n",
    "    (1): Linear(in_features=768, out_features=512, bias=True)\n",
    "    \n",
    "    (2): GELU(approximate='none')\n",
    "    \n",
    "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
    "    \n",
    "    (4): GELU(approximate='none')\n",
    "    \n",
    "    (5): Linear(in_features=512, out_features=22, bias=True)\n",
    "  \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            basemodel,\n",
    "            tagset_size,\n",
    "            lr=5e-5\n",
    "        ):\n",
    "        torch.manual_seed(1)\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.basemodel = basemodel\n",
    "        # the encoder will be a BERT-like model that receives an input text in subwords and maps each subword into\n",
    "        # contextual representations\n",
    "        self.encoder = None\n",
    "        # the hidden dimension of the BERT-like model will be automatically set in the init function!\n",
    "        self.encoder_hidden_dim = 0\n",
    "        # The linear layer that maps the subword contextual representation space to tag space\n",
    "        self.classification_head = None\n",
    "        # The CRF layer on top of the classification head to make sure the model learns to move from/to relevant tags\n",
    "        # self.crf_layer = None\n",
    "        # optimizers will be initialized in the init_model_from_scratch function\n",
    "        self.optimizers = None\n",
    "        self.init_model_from_scratch(basemodel, tagset_size, lr)\n",
    "\n",
    "    def init_model_from_scratch(self, basemodel, tagset_size, lr):\n",
    "        self.encoder = AutoModel.from_pretrained(basemodel)\n",
    "        self.encoder_hidden_dim = self.encoder.config.hidden_size\n",
    "        # self.classification_head = nn.Linear(self.encoder_hidden_dim, tagset_size)\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Dropout(.1, inplace=False),\n",
    "            nn.Linear( self.encoder_hidden_dim, 512 ),\n",
    "            nn.GELU(),\n",
    "            nn.Linear( 512, 512 ),\n",
    "            nn.GELU(),\n",
    "            nn.Linear( 512, tagset_size )\n",
    "        )\n",
    "\n",
    "        # TODO initialize self.crf_layer in here as well.\n",
    "        # TODO modify the optimizers in a way that each model part is optimized with a proper learning rate!\n",
    "        self.optimizers = [ \n",
    "            optim.Adam(\n",
    "                list(self.encoder.parameters()),\n",
    "                lr=lr\n",
    "            ),\n",
    "            optim.SGD( \n",
    "                list(self.classification_head.parameters() ) ,\n",
    "                lr= 0.1\n",
    "                      )\n",
    "        ]\n",
    "        \n",
    "        self.lr_schedulers = [\n",
    "            ExponentialLR(self.optimizers[0], 0.9),\n",
    "            ExponentialLR(self.optimizers[1], 0.9),\n",
    "        ]\n",
    "\n",
    "    def forward(self, sentence_input):\n",
    "        encoded = self.encoder(sentence_input).last_hidden_state\n",
    "        tag_space = self.classification_head(encoded)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
    "        # TODO modify the tag_scores to use the parameters of the crf_layer\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### Data augmentation\n",
    "Most of the improvement was obtained only buy augmenting training data.\n",
    "we have implemented data augmentation  dynamically, wich means every epoch, a new augmentation step is applied to the data, this  results in more varied training data.\n",
    "we achived accuracy of 96.4 and FB1 score of 94.54 only by data augmentation in the first place. Our initial data augmentation process involved only character replacement. However, we improved it by adding other types of text manipulations and got better accuracy.\n",
    "\n",
    "### Using seperate optimizer\n",
    "As it was proposed in the code, we have utilized a seperate optimizer for classification_head.\n",
    "it seems like the results are not much improved and using seperate optimizer had minimum effect on accuracy of the model.\n",
    "\n",
    "### Freezing the encoder parameters in the beginning\n",
    "We also tried freezing the encoder's weight for the first few epochs and training only the classification_head but it didn't go well and worsened the results\n",
    "\n",
    "### MLP classification head\n",
    "The classification head was modified as stated before, also we have increased number of training epochs to 20.\n",
    "these changes improved both accuracy and F1 score. accuracy = 96.77 -- FB1 = 94.91\n",
    "\n",
    "### Using learning rate scheduler\n",
    "We first halved the learning rate at epoch 15 for both optimizers which improved the FB1 score by almost 0.1. So we though it might be useful to use ExponentialLR schedulers from Pytorch with the gamma value of 0.9 to decay the learning rate throughout the whole training procedure starting from epoch 4. This idea also had a good impact on the accuracy.\n",
    "\n",
    "### Increasing the number of epochs\n",
    "We finally increased the number of epochs to 40 and reached the FB1 score of about 95.6."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
