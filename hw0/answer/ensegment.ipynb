{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensegment: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensegment import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default solution, we get the following result on the dev dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose spain\n",
      "this is a test\n",
      "who represents\n",
      "experts exchange\n",
      "speed of art\n",
      "unclimatechangebody\n",
      "we are the people\n",
      "mentionyourfaves\n",
      "now playing\n",
      "the walking dead\n",
      "follow me\n",
      "we are the people\n",
      "mentionyourfaves\n",
      "check domain\n",
      "big rock\n",
      "name cheap\n",
      "apple domains\n",
      "honesty hour\n",
      "being human\n",
      "follow back\n",
      "social media\n",
      "30secondstoearth\n",
      "current ratesoughttogodown\n",
      "this is insane\n",
      "what is my name\n",
      "is it time\n",
      "let us go\n",
      "me too\n",
      "nowthatcherisdead\n",
      "advice for young journalists\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abbove results show that the model failed to segments some words like \"unclimatechangebody\", \"mentionyourfaves\", etc. Interestingly enough, the words \"unclimatechangebody\" and \"mentionyourfaves\" do not exist in the count dictionary file \"count_1w.txt\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem arises from the fact that the probability of unknown words (unavailable in the \"count_1w.txt\") is considered 1/N in the code, where N is the total number of the words in the corpus. This can be seen in the \"Pdist\" class where the default \"missingfn\" is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.missingfn = missingfn or (lambda k, N: 1./N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phenomenon could cause the probability of some segmented nouns like \"un climate change body\" which is p1.p2.p3.p4 becomes less than 1/N (p1, p2, p3, p4 are the probability of \"un\", \"climate\", \"change\", \"body\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more reasonable approach is to consider the probability of the unknown words 0. In other words, we need to define the \"missingfn\" equal to 0, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose spain\n",
      "this is a test\n",
      "who represents\n",
      "experts exchange\n",
      "speed of art\n",
      "un climate change body\n",
      "we are the people\n",
      "mention your faves\n",
      "now playing\n",
      "the walking dead\n",
      "follow me\n",
      "we are the people\n",
      "mention your faves\n",
      "check domain\n",
      "big rock\n",
      "name cheap\n",
      "apple domains\n",
      "honesty hour\n",
      "being human\n",
      "follow back\n",
      "social media\n",
      "3 0 seconds to earth\n",
      "current rate sought to go down\n",
      "this is insane\n",
      "what is my name\n",
      "is it time\n",
      "let us go\n",
      "me too\n",
      "now thatcher is dead\n",
      "advice for young journalists\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=(lambda k, N: 0))\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, another problem arises, which is more apparent in the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h o w t o b r e a k u p i n 5 words\n",
      "what makes god smile\n",
      "1 0 people who mean alot to me\n",
      "w o r s t d a y i n 4 words\n",
      "l o v e s t o r y i n 5 words\n",
      "t o p 3 favourite comics\n",
      "1 0 breakup lines\n",
      "things that make you smile\n",
      "best female athlete\n",
      "w o r s t b o s s i n 5 words\n",
      "now is the time for all good\n",
      "it is a truth universally acknowledged\n",
      "when in the course of human events it becomes necessary\n",
      "it was a bright cold day in april and the clocks were striking thirteen\n",
      "it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\n",
      "as gregor samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect\n",
      "in a hole in the ground there lived a hobbit not a nasty dirty wet hole filled with the ends of worms and an oozy smell nor yet a dry bare sandy hole with nothing in it to sitdown on or to eat it was a hobbit hole and that means comfort\n",
      "far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the galaxy lies a small un regarded yellow sun\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=(lambda k, N: 0))\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/test.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is that once the recursive segmentation algorithm reaches an unknown word (or letter), it segments all of its left-side words into single letters. Because single-letter words are the last segmentation to be checked and are always been picked up in the max function although their probability is 0. For instance, look at \"h o w t o b r e a k u p i n 5 words\", in which \"5\" is an unknown word.\n",
    "\n",
    "Therefore, we decided to assign non-zero probability to the the unknown words again, but this time, we want this probability to be exponentially proportional to the length of the unknown words. \n",
    "Put differently, the probability of an unknown word with length 5 is much less than that of an unknown word with length 1. This approach can also prevent facing the issue we had in the first place with missingfn=(lambda k, N: 1./N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to breakup in 5 words\n",
      "what makes god smile\n",
      "1 0 people who mean alot to me\n",
      "worst day in 4 words\n",
      "love story in 5 words\n",
      "top 3 favourite comics\n",
      "1 0 breakup lines\n",
      "things that make you smile\n",
      "best female athlete\n",
      "worst boss in 5 words\n",
      "now is the time for all good\n",
      "it is a truth universally acknowledged\n",
      "when in the course of human events it becomes necessary\n",
      "it was a bright cold day in april and the clocks were striking thirteen\n",
      "it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness\n",
      "as gregor samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect\n",
      "in a hole in the ground there lived a hobbit not a nasty dirty wet hole filled with the ends of worms and an oozy smell nor yet a dry bare sandy hole with nothing in it to sitdown on or to eat it was a hobbit hole and that means comfort\n",
      "far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the galaxy lies a small un regarded yellow sun\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=(lambda k, N: pow(1./N, len(k))))\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"../data/input/test.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the missingfn=(lambda k, N: pow(1./N, len(k))), the segmentations are much better and the model is able to capture the correct segments in almost all the test data.\n",
    "We used the same approach and reached .98 F-score on the dev dataset.\n",
    "So, this is our final submission for HW0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
