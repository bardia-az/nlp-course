{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prefixtune: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/sajad/New Volume/projects/mirzaei/nlp-class-hw/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from default import *\n",
    "import os, sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:02,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0||  ___________________________________________________________________________    This is the latest in a series of articles on the evolution of the internet in the United States and Europe.  In the United States, the United States has become the world's largest internet service provider. \n",
      "1||  __________________________________________________________________________    We’ve been working hard to improve the quality of our products for the past few years. We’ve been working hard to improve the quality of our products for the past few years. We\n",
      "2||  _______________________________________________   The following is a blog post on my blog. The following is a blog post on my blog. The following is a blog post on my blog. The following is a blog post on my blog. \n",
      "3||  中国自己自己自己自己自己自己自己自己自己�\n",
      "4||  _____________________   The following is a list of the most common and most common reasons why you should buy a laptop or laptop.  1. The first thing you need to know is that you need to buy a laptop or laptop. \n",
      "5||  __________________________________________________________________________    A few weeks ago, I wrote a post on my blog that I wanted to share with you a bit more about the history of the Internet and the Internet. I’ve been writing about the internet for a\n",
      "6||  ___________   For the first time in a long time, the US Senate has passed a bill to repeal and replace the Affordable Care Act.   The House of Representatives passed a bill to repeal and replace the Affordable Care Act. \n",
      "7||  __________________________________________________________________________   This is what you need to know when you need to make a change to your daily life.  The following is a guide to the most important changes in your life. 1.1.1.1.\n",
      "8||   If you're looking for the best way to get your hands on the latest Linux version of Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux\n",
      "9||  ***********    The following is an excerpt from the latest edition of the New York Times bestselling book, The New York Times Book of the Year.  The New York Times Book of the Year is a must-read for anyone who\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "basemodel = 'distilgpt2'\n",
    "table_to_text = TableToText(\"peft\", basemodel=basemodel)\n",
    "model = AutoModelForCausalLM.from_pretrained(basemodel)\n",
    "model.to(device)\n",
    "decoder_output = table_to_text.decode(model, '../data/input/small.txt')\n",
    "print(\"\\n\".join(decoder_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the warnings from the transformers library. They are expected to occur."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score: 1.0794768613240795\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "bleu = sacrebleu.metrics.BLEU(effective_order=True)\n",
    "\n",
    "def compute_bleu(references, output_data):\n",
    "    bleu_score = 0.0\n",
    "    if len(references) == len(output_data):\n",
    "        score = 0.0\n",
    "        total = 0.0\n",
    "        for line in output_data:\n",
    "            r = references[line[0]]\n",
    "            h = line[1]\n",
    "            score += bleu.sentence_score(h, r).score\n",
    "            total += 1.\n",
    "        bleu_score = score / total\n",
    "    return bleu_score\n",
    "\n",
    "output = \"\\n\".join(decoder_output)\n",
    "\n",
    "references = {}\n",
    "ref_data = []\n",
    "with open( '../data/reference/small.out', 'r') as ref:\n",
    "    ref_data = list(filter(lambda k: k, [str(x) for x in ref.read().splitlines()]))\n",
    "    for line in ref_data:\n",
    "        src_id, _, suggested_reference = line.split('||')\n",
    "        references.setdefault(src_id, [])\n",
    "        references[src_id].append(suggested_reference)\n",
    "\n",
    "output_data = list(filter(lambda k: k, [str(x) for x in output.splitlines()]))\n",
    "output_data = [line.split('||') for line in output_data]\n",
    "output_data = output_data[:len(ref_data)]\n",
    "\n",
    "print(f\"bleu score: {compute_bleu(references, output_data)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `bleu.py`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "We used the PeftModel from transformers library which, according to the documentation (https://huggingface.co/docs/peft/package_reference/peft_model) is the base model class for specifying the base Transformer model and configuration to apply a PEFT method to it.\n",
    "PEFT stands for Parameter-Efficient Fine-Tuning which is a framework to facilitate different fine-tuning under this category. The implementaion is as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, PrefixTuningConfig, PeftModel,  TaskType, PeftConfig\n",
    "\n",
    "def train(self):\n",
    "        data_loaders = self.get_data(splits=(\"train\", ))\n",
    "        model = AutoModelForCausalLM.from_pretrained(self.basemodel)\n",
    "        # print('train loop cdalled')\n",
    "        # You can print the parameters for debugging or understanding the code\n",
    "        # but make sure you comment it out otherwise it will pollute the output\n",
    "        # that is produced for dev and test\n",
    "        #model.print_trainable_parameters()\n",
    "        peft_config = PrefixTuningConfig( task_type= TaskType.CAUSAL_LM ,prefix_projection= self.prefixprojection , inference_mode=False, num_virtual_tokens= self.virtualtokens)\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        # model.print_trainable_parameters()\n",
    "        # TODO\n",
    "        # if using HF peft module, then add calls to PrefixTuningConfig and get_peft_model\n",
    "        # which will take num_virtual_tokens which is set to self.virtualtokens and\n",
    "        # prefix_projection which is set to self.prefixprojection\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr)\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=(len(data_loaders[\"train\"]) * self.epochs),\n",
    "        )\n",
    "        model = model.to(device)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            # print(f'epoch {epoch}')\n",
    "            # TODO rest of the training steps for prefix tuning\n",
    "            for step, batch in enumerate( tqdm( data_loaders['train'] ) ):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                # print(f'step {step}')\n",
    "\n",
    "            if epoch == self.epochs - 1:\n",
    "                epoch_str = '' # last epoch so do not use epoch number in model filename\n",
    "            else:\n",
    "                epoch_str = str(epoch)\n",
    "            savefile = self.modelfile + epoch_str + self.modelsuffix\n",
    "            model.save_pretrained(savefile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we didn't need to set model.training variable in every step, it's been moved outside the loop. \n",
    "\n",
    "We can load the pretrained model with AutoModelForCausalLM.from_pretrained method then we need to create a proper config obj by calling PrefixTuningConfig and pass the parameters accordingly, now we can instanciate PeftModel which is a wrapper around the loaded model. This wrapper freezes the model weights and adds some layers depending to the passed config which in this case implements prefix tuning.\n",
    "\n",
    "Also, the output contains losses accumulated by crossentrpy loss which we can backpropagate through the network.\n",
    "\n",
    "Finally, after training, related information about additional layers and structures created around the original model is saved in a predefined directory. We can reload this information by calling the following methods:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(  modelfile + opts.modelsuffix  )\n",
    "model = AutoModelForCausalLM.from_pretrained( opts.basemodel )\n",
    "model = PeftModel.from_pretrained( model,  modelfile + opts.modelsuffix  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also because of limited memory resources on GPU, we reduces batch size to 12."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "just implementing the prefix fine tuning is not enough and the model is not able to produce relevante information as we can see in small tests results:\n",
    "\n",
    "    0||  The Alimentum is located in the riverside area near the riverside area near the riverside area near the riverside area near the riverside area near the riverside area near the riverside area near the riverside area near the riverside\n",
    "    \n",
    "By implementing the prefix fine tuning the bleu score improved to : 14.234. The output is no longer irrelevant compared to the default settings, but suffers from duplicate n-grams and incomplete information.\n",
    "So we have changed prefixprojection to True.\n",
    "it mildly improved bleu score to 14.79 but the repeating n-grams still exist. so without retraining, we added no_repeat_ngram_size=2, to the generate method, and bleu score changed to 8.44 on the small test set, but the irrelevant data came back into context. for example:\n",
    "\n",
    "    0||  Alimentums in the city center is located near the riverside. It is a child friendly place with a customer rating of 5.  The children friendly atmosphere with the price range of £20-£ range. Located near to the river, Al\n",
    "\n",
    "So, we thought maybe 50 new tokens were too much. in the next step we decreased max_new_tokens and tested it again. It almost doubled the bleu score i.e.BLEU score : 15.18\n",
    "\n",
    "The next step was to increase beam width, we increased it to 10 and in the meantime increased temperature so that more combinations could be generated by the model, it also increased BLEU score up to 21%.\n",
    "We've tested with different virtualtokens but it does not make much of a difference.\n",
    "\n",
    "Furthermore, when we have a big number for new tokens and we reduce the probability of repeating n-grams to 0, the model starts to generate meaningful but unrelated texts, so the question is which combination of these two is proper for achieving a good result. After some trial and error, it seems like 30 new tokens and 6-gram 0 repeatation is a good point which resulted in a 40 BLEU score for the dev set and a 33 BLEU score for the small test set.\n",
    "\n",
    " ### Here is the final generate method parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens= 30,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer_pad_token_id,\n",
    "            do_sample=True,\n",
    "            num_beams=10,\n",
    "            top_p=0.9,\n",
    "            temperature= 1.5,\n",
    "            no_repeat_ngram_size= 6,\n",
    "            num_return_sequences=num_sequences\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
