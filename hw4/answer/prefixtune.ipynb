{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prefixtune: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bardia_az/nlp/nlpclass-1237-g-savagetokenizer/hw4/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from default import *\n",
    "import os, sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:07,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0||  ___________   A new book is coming out.   A new book is coming out.  A new book is coming out. A new book is coming out. A new book is coming out. A new\n",
      "1||  ____________    I’ve been working on this project for a while now, and I’ve been working on this project for a while now, and I’ve been working on this project for a while now,\n",
      "2||  __________________________________________   A few years ago, I was going to make a list of the most important things you will need to know to know about Bitcoin and Bitcoin. Bitcoin is the world's first digital currency.  Bitcoin is a\n",
      "3||  __________________________________________________________________________    The U.S. Department of Homeland Security (DHS) announced today that the Department of Homeland Security (DHS) has approved the release of a new report released by the Department of Homeland Security (DHS\n",
      "4||  ____________________________________________   If you've been in the news for the past few years, you've been wondering what's going on in the world of video games, especially video games. It's time for you to take a closer look at the\n",
      "5||  _______________________________________________________   I’ve been working on this blog for a while now, and I’ve been working on this blog for a while now, and I’ve been working on this blog for a while now,\n",
      "6||  __________________________________________    The U.S. Department of Agriculture (USDA) has issued a directive to the U.S. Department of Agriculture (USDA) that the U.S. Department of Agriculture (USDA) should\n",
      "7||  中文文本語 中文本語 中文本語 中文本語 中文本語 中文本語 \n",
      "8||  __________________________________________________________________________________________________________________   The following is a list of things you should know about the Apple Watch.  1. How to use the Apple Watch. The Apple Watch is an Apple Watch. The Apple Watch is an Apple Watch\n",
      "9||  __________________________________________________________________________________   If you are looking for a way to get a better understanding of what's going on in your life, here are some tips to help you find a better way to get a better understanding of what's going on in your\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "basemodel = 'distilgpt2'\n",
    "table_to_text = TableToText(\"peft\", basemodel=basemodel)\n",
    "model = AutoModelForCausalLM.from_pretrained(basemodel)\n",
    "model.to(device)\n",
    "decoder_output = table_to_text.decode(model, '../data/input/small.txt')\n",
    "print(\"\\n\".join(decoder_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the warnings from the transformers library. They are expected to occur."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "bleu = sacrebleu.metrics.BLEU(effective_order=True)\n",
    "\n",
    "def compute_bleu(references, output_data):\n",
    "    bleu_score = 0.0\n",
    "    if len(references) == len(output_data):\n",
    "        score = 0.0\n",
    "        total = 0.0\n",
    "        for line in output_data:\n",
    "            r = references[line[0]]\n",
    "            h = line[1]\n",
    "            score += bleu.sentence_score(h, r).score\n",
    "            total += 1.\n",
    "        bleu_score = score / total\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score: 0.9298078296758284\n"
     ]
    }
   ],
   "source": [
    "output = \"\\n\".join(decoder_output)\n",
    "\n",
    "references = {}\n",
    "ref_data = []\n",
    "with open( '../data/reference/small.out', 'r') as ref:\n",
    "    ref_data = list(filter(lambda k: k, [str(x) for x in ref.read().splitlines()]))\n",
    "    for line in ref_data:\n",
    "        src_id, _, suggested_reference = line.split('||')\n",
    "        references.setdefault(src_id, [])\n",
    "        references[src_id].append(suggested_reference)\n",
    "\n",
    "output_data = list(filter(lambda k: k, [str(x) for x in output.splitlines()]))\n",
    "output_data = [line.split('||') for line in output_data]\n",
    "output_data = output_data[:len(ref_data)]\n",
    "\n",
    "print(f\"bleu score: {compute_bleu(references, output_data)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `bleu.py`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "We used the PeftModel from transformers library which, according to the documentation (https://huggingface.co/docs/peft/package_reference/peft_model) is the base model class for specifying the base Transformer model and configuration to apply a PEFT method to it.\n",
    "PEFT stands for Parameter-Efficient Fine-Tuning which is a framework to facilitate different fine-tuning under this category. The implementaion is as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, PrefixTuningConfig, PeftModel,  TaskType, PeftConfig\n",
    "\n",
    "def train(self):\n",
    "        data_loaders = self.get_data(splits=(\"train\", ))\n",
    "        model = AutoModelForCausalLM.from_pretrained(self.basemodel)\n",
    "\n",
    "        # You can print the parameters for debugging or understanding the code\n",
    "        # but make sure you comment it out otherwise it will pollute the output\n",
    "        # that is produced for dev and test\n",
    "        #model.print_trainable_parameters()\n",
    "        \n",
    "        peft_config = PrefixTuningConfig( task_type= TaskType.CAUSAL_LM ,prefix_projection= self.prefixprojection , inference_mode=False, num_virtual_tokens= self.virtualtokens)\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        # model.print_trainable_parameters()\n",
    "\n",
    "        # TODO\n",
    "        # if using HF peft module, then add calls to PrefixTuningConfig and get_peft_model\n",
    "        # which will take num_virtual_tokens which is set to self.virtualtokens and\n",
    "        # prefix_projection which is set to self.prefixprojection\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr)\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=(len(data_loaders[\"train\"]) * self.epochs),\n",
    "        )\n",
    "        model = model.to(device)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            # TODO rest of the training steps for prefix tuning\n",
    "            for step, batch in enumerate( tqdm( data_loaders['train'] ) ):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if epoch == self.epochs - 1:\n",
    "                epoch_str = '' # last epoch so do not use epoch number in model filename\n",
    "            else:\n",
    "                epoch_str = str(epoch)\n",
    "            savefile = self.modelfile + epoch_str + self.modelsuffix\n",
    "            model.save_pretrained(savefile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we didn't need to set model.training variable in every step, it's been moved outside the loop. \n",
    "\n",
    "We can load the pretrained model with AutoModelForCausalLM.from_pretrained method then we need to create a proper config obj by calling PrefixTuningConfig and pass the parameters accordingly, now we can instanciate PeftModel which is a wrapper around the loaded model. This wrapper freezes the model weights and adds some layers depending to the passed config which in this case implements prefix tuning.\n",
    "\n",
    "Also the output contains losses accumulated by crossentropy loss that can be backpropagated through the network.\n",
    "\n",
    "Because of limited memory resources on GPU, we alse reduced batch size to 12.\n",
    "\n",
    "finally, after training, related information about additional layers and structure created around the original model is saved in a predefined directory. we can reload this information by calling following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-5): 6 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PrefixEncoder(\n",
       "      (embedding): Embedding(10, 9216)\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(50257, 768)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelfile = '../data/peft'\n",
    "modelsuffix = '.pt'\n",
    "basemodel = 'distilgpt2'\n",
    "config = PeftConfig.from_pretrained(modelfile + modelsuffix)\n",
    "model = AutoModelForCausalLM.from_pretrained(basemodel)\n",
    "model = PeftModel.from_pretrained(model, modelfile + modelsuffix)\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "just implementing the prefix fine tuning is not enough and the model is not able to produce relevante information as we can see in small tests results:\n",
    "\n",
    "    0||  The Alimentum is located in the riverside area near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near the city\n",
    "and the bleu score improved to : 13.648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:05,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0||  The Alimentum is located in the riverside area near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near the city\n",
      "1||  Alimentum is a restaurant located near the riverside near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near the city\n",
      "2||  Alimentum is a place in the city centre near the riverside near the riverside near the riverside near the riverside near the riverside near the riverside near the riverside near the riverside near the riverside near the riverside\n",
      "3||  Alimentum is located near Burger King in the riverside area near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near the city centre near\n",
      "4||  Alimentum is located in riverside near the riverside near the riverside near the riverside near the riverside near the riverside near the riverside near the riverside near the riverside near the riverside near the riverside near the\n",
      "5||  Alimentum is a place near Burger King in the riverside near Burger King in the riverside near Burger King in the riverside near Burger King in the riverside near Burger King in the riverside near Burger King in the riverside near Burger\n",
      "6||  The Alimentum is located in riverside near the riverside near the riverside near the riverside near the riverside near the riverside near the riverside near the riverside near the riverside near the riverside near the riverside near\n",
      "7||  Alimentum is located near Burger King in the riverside near Burger King in the riverside near Burger King in the riverside near Burger King in the riverside near Burger King in the riverside near Burger King in the riverside near Burger King\n",
      "8||  There is a coffee shop called Aromi located in riverside near the riverside near the riverside near the riverside near the riverside near the riverside. It is located near the riverside near the riverside near the riverside near\n",
      "9||  There is a coffee shop called Aromi that serves Chinese food in riverside. It is located in the riverside area. It is located in the riverside area. It is located in the riverside area. It is located in the rivers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "decoder_output = table_to_text.decode(model, '../data/input/small.txt')\n",
    "print(\"\\n\".join(decoder_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score: 13.648864452792433\n"
     ]
    }
   ],
   "source": [
    "output = \"\\n\".join(decoder_output)\n",
    "\n",
    "references = {}\n",
    "ref_data = []\n",
    "with open( '../data/reference/small.out', 'r') as ref:\n",
    "    ref_data = list(filter(lambda k: k, [str(x) for x in ref.read().splitlines()]))\n",
    "    for line in ref_data:\n",
    "        src_id, _, suggested_reference = line.split('||')\n",
    "        references.setdefault(src_id, [])\n",
    "        references[src_id].append(suggested_reference)\n",
    "\n",
    "output_data = list(filter(lambda k: k, [str(x) for x in output.splitlines()]))\n",
    "output_data = [line.split('||') for line in output_data]\n",
    "output_data = output_data[:len(ref_data)]\n",
    "\n",
    "print(f\"bleu score: {compute_bleu(references, output_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output is not irrelevant anymore compaire to default settings but they contain repeating n-grams and incomplete informations.\n",
    "so we have changed prefixprojection to True\n",
    "it mildly improved bleu score to bleu score: 14.79 but the repeating n-grams still exists. so without retraining, we added no_repeat_ngram_size=2, to the generate method and bleu score increas to 8.44 on the small test set, but the irrelevant data came back into context. for example:\n",
    "    0||  Alimentums in the city center is located near the riverside. It is a child friendly place with a customer rating of 5.  The children friendly atmosphere with the price range of £20-£ range. Located near to the river, Al\n",
    "\n",
    "So, we thought maybe 50 new tokens were too much. in the next step we decreased max_new_tokens and tested it again. It almost doubled the bleu score i.e.BLEU score : 15.18\n",
    "\n",
    "The next step was to increase beam width, we increased it to 10 and in the meantime increased temperature so that more combinations could be generated by the model, it also increased BLEU score up to 21%.\n",
    "We've tested with different virtualtokens but it does not make much of a difference.\n",
    "\n",
    "Furthermore, when we have a big number for new tokens and we reduce the probability of repeating n-grams to 0, the model starts to generate meaningful but unrelated texts, so the question is which combination of these two is proper for achieving a good result. After some trial and error, it seems like 30 new tokens and 6-gram 0 repeatation is a good point which resulted in a 40 BLEU score for the dev set and a 33 BLEU score for the small test set.\n",
    "\n",
    " ### Here is the final generate method parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not run this cell\n",
    "outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens= 30,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer_pad_token_id,\n",
    "            do_sample=True,\n",
    "            num_beams=10,\n",
    "            top_p=0.9,\n",
    "            temperature= 1.5,\n",
    "            no_repeat_ngram_size= 6,\n",
    "            num_return_sequences=num_sequences\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/bardia_az/nlp/nlpclass-1237-g-savagetokenizer/hw4/venv/lib/python3.11/site-packages/peft/peft_model.py:1081: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n",
      "10it [00:04,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0||  The Alimentum is located in the city centre. It is located in the riverside area near the riverside area. It is located near the\n",
      "1||  Alimentum is located near Burger King in the riverside area near the riverside area near Burger King. It is located near Burger King. It\n",
      "2||  The Alimentum is located in the city centre near the riverside near the riverside. Located near the riverside, Alimentum is a\n",
      "3||  Alimentum is located near Burger King in the riverside area near Burger King. It is located near Burger King and has a customer rating of 5\n",
      "4||  The Alimentum is located in the riverside area near the riverside near the riverside. The Alimentum is located near the riverside\n",
      "5||  Alimentum is located near Burger King in the riverside near Burger King. It is located near Burger King, located near the riverside near Burger\n",
      "6||  Alimentum is located in the riverside near the riverside near the city centre. Alimentum is located near the city centre near the city\n",
      "7||  Alimentum is located near Burger King in the riverside near Burger King. It is located near Burger King, located near the riverside near Burger\n",
      "8||  Aromi is a coffee shop located in riverside that serves Chinese food. It has a customer rating of 1 out of 5 and is located in\n",
      "9||  Located near the riverside near the riverside with a customer rating of 3 out of 5, it is located near the riverside, it is located\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from prefixtune import TableToText\n",
    "table_to_text = TableToText(\"peft\", basemodel=basemodel)\n",
    "decoder_output = table_to_text.decode(model, '../data/input/small.txt')\n",
    "print(\"\\n\".join(decoder_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score: 33.85715455043608\n"
     ]
    }
   ],
   "source": [
    "output = \"\\n\".join(decoder_output)\n",
    "\n",
    "references = {}\n",
    "ref_data = []\n",
    "with open( '../data/reference/small.out', 'r') as ref:\n",
    "    ref_data = list(filter(lambda k: k, [str(x) for x in ref.read().splitlines()]))\n",
    "    for line in ref_data:\n",
    "        src_id, _, suggested_reference = line.split('||')\n",
    "        references.setdefault(src_id, [])\n",
    "        references[src_id].append(suggested_reference)\n",
    "\n",
    "output_data = list(filter(lambda k: k, [str(x) for x in output.splitlines()]))\n",
    "output_data = [line.split('||') for line in output_data]\n",
    "output_data = output_data[:len(ref_data)]\n",
    "\n",
    "print(f\"bleu score: {compute_bleu(references, output_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
