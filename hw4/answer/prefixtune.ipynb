{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prefixtune: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/sajad/New Volume/projects/mirzaei/nlp-class-hw/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from default import *\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:02,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0||  ___________________________________________________________________________    This is the latest in a series of articles on the evolution of the internet in the United States and Europe.  In the United States, the United States has become the world's largest internet service provider. \n",
      "1||  __________________________________________________________________________    We’ve been working hard to improve the quality of our products for the past few years. We’ve been working hard to improve the quality of our products for the past few years. We\n",
      "2||  _______________________________________________   The following is a blog post on my blog. The following is a blog post on my blog. The following is a blog post on my blog. The following is a blog post on my blog. \n",
      "3||  中国自己自己自己自己自己自己自己自己自己�\n",
      "4||  _____________________   The following is a list of the most common and most common reasons why you should buy a laptop or laptop.  1. The first thing you need to know is that you need to buy a laptop or laptop. \n",
      "5||  __________________________________________________________________________    A few weeks ago, I wrote a post on my blog that I wanted to share with you a bit more about the history of the Internet and the Internet. I’ve been writing about the internet for a\n",
      "6||  ___________   For the first time in a long time, the US Senate has passed a bill to repeal and replace the Affordable Care Act.   The House of Representatives passed a bill to repeal and replace the Affordable Care Act. \n",
      "7||  __________________________________________________________________________   This is what you need to know when you need to make a change to your daily life.  The following is a guide to the most important changes in your life. 1.1.1.1.\n",
      "8||   If you're looking for the best way to get your hands on the latest Linux version of Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux Mint Linux\n",
      "9||  ***********    The following is an excerpt from the latest edition of the New York Times bestselling book, The New York Times Book of the Year.  The New York Times Book of the Year is a must-read for anyone who\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "basemodel = 'distilgpt2'\n",
    "table_to_text = TableToText(\"peft\", basemodel=basemodel)\n",
    "model = AutoModelForCausalLM.from_pretrained(basemodel)\n",
    "model.to(device)\n",
    "decoder_output = table_to_text.decode(model, '../data/input/small.txt')\n",
    "print(\"\\n\".join(decoder_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the warnings from the transformers library. They are expected to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score: 1.0794768613240795\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "bleu = sacrebleu.metrics.BLEU(effective_order=True)\n",
    "\n",
    "def compute_bleu(references, output_data):\n",
    "    bleu_score = 0.0\n",
    "    if len(references) == len(output_data):\n",
    "        score = 0.0\n",
    "        total = 0.0\n",
    "        for line in output_data:\n",
    "            r = references[line[0]]\n",
    "            h = line[1]\n",
    "            score += bleu.sentence_score(h, r).score\n",
    "            total += 1.\n",
    "        bleu_score = score / total\n",
    "    return bleu_score\n",
    "\n",
    "output = \"\\n\".join(decoder_output)\n",
    "\n",
    "references = {}\n",
    "ref_data = []\n",
    "with open( '../data/reference/small.out', 'r') as ref:\n",
    "    ref_data = list(filter(lambda k: k, [str(x) for x in ref.read().splitlines()]))\n",
    "    for line in ref_data:\n",
    "        src_id, _, suggested_reference = line.split('||')\n",
    "        references.setdefault(src_id, [])\n",
    "        references[src_id].append(suggested_reference)\n",
    "\n",
    "output_data = list(filter(lambda k: k, [str(x) for x in output.splitlines()]))\n",
    "output_data = [line.split('||') for line in output_data]\n",
    "output_data = output_data[:len(ref_data)]\n",
    "\n",
    "print(f\"bleu score: {compute_bleu(references, output_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `bleu.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "we used the PeftModel from transformers library wich, according to the documentation (https://huggingface.co/docs/peft/package_reference/peft_model) \"is the base model class for specifying the base Transformer model and configuration to apply a PEFT method to.\", PEFT stands for Parameter-Efficient Fine-Tuning wich is a frame work to facilitate diffrent fine tuning under this category. the implementaion is as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, PrefixTuningConfig, PeftModel,  TaskType, PeftConfig\n",
    "\n",
    "def train(self):\n",
    "        data_loaders = self.get_data(splits=(\"train\", ))\n",
    "        model = AutoModelForCausalLM.from_pretrained(self.basemodel)\n",
    "        # print('train loop cdalled')\n",
    "        # You can print the parameters for debugging or understanding the code\n",
    "        # but make sure you comment it out otherwise it will pollute the output\n",
    "        # that is produced for dev and test\n",
    "        #model.print_trainable_parameters()\n",
    "        peft_config = PrefixTuningConfig( task_type= TaskType.CAUSAL_LM ,prefix_projection= self.prefixprojection , inference_mode=False, num_virtual_tokens= self.virtualtokens)\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        # model.print_trainable_parameters()\n",
    "        # TODO\n",
    "        # if using HF peft module, then add calls to PrefixTuningConfig and get_peft_model\n",
    "        # which will take num_virtual_tokens which is set to self.virtualtokens and\n",
    "        # prefix_projection which is set to self.prefixprojection\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr)\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=(len(data_loaders[\"train\"]) * self.epochs),\n",
    "        )\n",
    "        model = model.to(device)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            # print(f'epoch {epoch}')\n",
    "            # TODO rest of the training steps for prefix tuning\n",
    "            for step, batch in enumerate( tqdm( data_loaders['train'] ) ):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                # print(f'step {step}')\n",
    "\n",
    "            if epoch == self.epochs - 1:\n",
    "                epoch_str = '' # last epoch so do not use epoch number in model filename\n",
    "            else:\n",
    "                epoch_str = str(epoch)\n",
    "            savefile = self.modelfile + epoch_str + self.modelsuffix\n",
    "            model.save_pretrained(savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we didn't need to set model.training variable in every step, it's been moved outside the loop. \n",
    "\n",
    "we can load the pretrained model with AutoModelForCausalLM.from_pretrained method\n",
    "then we need to creat proper config obj with calling PrefixTuningConfig and pass the paramters accordingly,then we can instanciate PeftModel wich is a wrapper around loaded model. this wrapper freezes the model weights and adds some layers depending to the passed config wich in this case implements prefix tuning.\n",
    "\n",
    "also the output contains losses accumulated by crossentrpy loss wich we can backpropagate through the network.\n",
    "\n",
    "finally, after training, related information about additionakl layers and structure created around the original model is saved in a predefined directory. we can reload this information by calling following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(  modelfile + opts.modelsuffix  )\n",
    "model = AutoModelForCausalLM.from_pretrained( opts.basemodel )\n",
    "model = PeftModel.from_pretrained( model,  modelfile + opts.modelsuffix  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also because of limited memory resources on GPU, we reduces batch size to 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "just implementing the prefix fine tuning is not enough and the model is not able to produce relevante information as we can see in small tests results:\n",
    "    0||  The Alimentum is located in the riverside area near the riverside area near the riverside area near the riverside area near the riverside area near the riverside area near the riverside area near the riverside area near the riverside\n",
    "and the bleu score improved to : 14.234\n",
    "the output is not irrelevant anymore compaire to default settings but they contain repeating n-grams and incomplete informations.\n",
    "so we have changed prefixprojection to True\n",
    "it mildly improved bleu score to bleu score: 14.79 but the repeating n-grams still exists. so without retraining, we added no_repeat_ngram_size=2, to the generate method and bleu score increas to 8.44 on the small test set, but the irrelevant data came back into context. for example:\n",
    "    0||  Alimentums in the city center is located near the riverside. It is a child friendly place with a customer rating of 5.  The children friendly atmosphere with the price range of £20-£ range. Located near to the river, Al\n",
    "\n",
    "so, maybe 50 new tokens were too much. in the next step we decreased max_new_tokens and tested it again. and it almost doubled the bleu score.\n",
    "BLEU score : 15.18\n",
    "\n",
    "the next step was to increas beam width, we increased it to 10 and in the mean time increased tempreture so that more combinations could be generated by the model, it also increased BLEU score up to 21%.\n",
    " we've test with diffrent virtualtokens but it does not makes much of a diffrence.\n",
    "\n",
    "furthur more, when we have a big number for new tokens and we reduce probability of repeating n-grams to 0, model starts to generate meaningfull but unrelatated texts, so the question is wich combination of this two are proper for achiving a good result. after some trial and error, it seams like 30 new tokens and 6-gram 0 repeatation is a good point wich resulted in 40 BLEU score for dev set and 33 BLEU score for small test set.\n",
    "\n",
    "###Here is the final generate method parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens= 30,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer_pad_token_id,\n",
    "            do_sample=True,\n",
    "            num_beams=10,\n",
    "            top_p=0.9,\n",
    "            temperature= 1.5,\n",
    "            no_repeat_ngram_size= 6,\n",
    "            num_return_sequences=num_sequences\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
