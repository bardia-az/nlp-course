{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spellchk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Read `answer/default.py` starting with the `spellchk` function and see how it solves the task of spell correction using a pre-trained language model that can predict a replacement token for a masked token in the input.\n",
    "\n",
    "In your submission, write some beautiful documentation of your program here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our program uses a pre-trained language model that provides 1000 candidates to replace the typo word. Then, we employ Levenshtein distance (edit distance) between the typo and the candidates. This distance is in fact the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. It basically measures how much a candidate is close to the typo word. We convert this distance to a number between 0 and 1, which resembles a probability number in order to be able to combine it with the candidates' score generated by the language model. These two probability numbers are combined together with a weight, and the candidate with the highest score will be selected.\n",
    "Note that the utilized language model is uncased, and all the generated candidates are lower case. In the sentences however, some of the typo words need to be capital. Therefore, we capitalize the final selected candidate if the typo word is also capital. In the following, you can see the changes we made to the \"spellchk\" and \"select_correction\" functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_correction(typo, predict):\n",
    "    # return the most likely prediction for the mask token\n",
    "    LM_score = np.array([predict[i]['score'] for i in range(len(predict))])\n",
    "    edit_dist = np.array([Levenshtein.distance(typo.lower(), predict[i]['token_str']) for i in range(len(predict))])\n",
    "    edit_score = (edit_dist.max() - edit_dist) / edit_dist.max()\n",
    "    w = 0.95\n",
    "    score_tot = w * edit_score + (1-w) * LM_score\n",
    "    best_ind = np.argmax(score_tot)\n",
    "    return predict[best_ind]['token_str'].capitalize() if typo[0].isupper() else predict[best_ind]['token_str']\n",
    "\n",
    "def spellchk(fh):\n",
    "    for (locations, sent) in get_typo_locations(fh):\n",
    "        spellchk_sent = sent\n",
    "        for i in locations:\n",
    "            # predict top_k replacements only for the typo word at index i\n",
    "            predict = fill_mask(\n",
    "                \" \".join([ sent[j] if j != i else mask for j in range(len(sent)) ]), \n",
    "                top_k=1000\n",
    "            )\n",
    "            logging.info(predict)\n",
    "            spellchk_sent[i] = select_correction(sent[i], predict)\n",
    "        yield(locations, spellchk_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "In the default solution, \"select_correction\" function just select the first candidate of the language model with the highest score. In fact, this approach does not take the typo word into account at all. It is not surprising that its accuracy is as low as 0.23 on the dev dataset. Here are the default \"spellchk\" and \"select_correction\" functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_correction(typo, predict):\n",
    "    # return the most likely prediction for the mask token\n",
    "    return predict[0]['token_str']\n",
    "\n",
    "def spellchk(fh):\n",
    "    for (locations, sent) in get_typo_locations(fh):\n",
    "        spellchk_sent = sent\n",
    "        for i in locations:\n",
    "            # predict top_k replacements only for the typo word at index i\n",
    "            predict = fill_mask(\n",
    "                \" \".join([ sent[j] if j != i else mask for j in range(len(sent)) ]), \n",
    "                top_k=20\n",
    "            )\n",
    "            logging.info(predict)\n",
    "            spellchk_sent[i] = select_correction(sent[i], predict)\n",
    "        yield(locations, spellchk_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\tit will put your mind into non-stop learning.\n",
      "3\t`` Sad '' s not the right word , of course .\n",
      "5,14\tJust before Myra left -- cathy was saying good-by to Cathy , and she did realize I was near '' .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with StringIO(\"4\\tit will put your maind into non-stop learning.\\n \\\n",
    "              3\\t`` Sad '' wss not the right word , of course .\\n \\\n",
    "              5,14\tJust before Myra left -- Sue was saying good-by to Cathy , and she didm't realize I was near '' .\") as f:\n",
    "    for (locations, spellchk_sent) in spellchk(f):\n",
    "        print(\"{locs}\\t{sent}\".format(\n",
    "            locs=\",\".join([str(i) for i in locations]),\n",
    "            sent=\" \".join(spellchk_sent)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the program replaces \"wss\" with \"s\" and \"Sue\" with \"cathy\" which has nothing to do with the typo word.\n",
    "\n",
    "So, in our first attempt, we counted the number of common words between the typo and the candidates using the set intersection and selected the one with highest number of common words. This approach improved the accuracy on the dev dataset to 0.46, but it is still low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_correction(typo, predict):\n",
    "    # return the most likely prediction for the mask token\n",
    "    dist_score = [len(set(typo.lower()).intersection(set(predict[i]['token_str'])))/len(typo) for i in range(len(predict))]\n",
    "    best_ind = np.argmax(dist_score)\n",
    "    return predict[best_ind]['token_str']\n",
    "\n",
    "def spellchk(fh):\n",
    "    for (locations, sent) in get_typo_locations(fh):\n",
    "        spellchk_sent = sent\n",
    "        for i in locations:\n",
    "            # predict top_k replacements only for the typo word at index i\n",
    "            predict = fill_mask(\n",
    "                \" \".join([ sent[j] if j != i else mask for j in range(len(sent)) ]), \n",
    "                top_k=20\n",
    "            )\n",
    "            logging.info(predict)\n",
    "            spellchk_sent[i] = select_correction(sent[i], predict)\n",
    "        yield(locations, spellchk_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\tit will put your mind into non-stop learning.\n",
      "3\t`` Sad '' was not the right word , of course .\n",
      "5,14\tJust before Myra left -- susie was saying good-by to Cathy , and she immediately realize I was near '' .\n"
     ]
    }
   ],
   "source": [
    "with StringIO(\"4\\tit will put your maind into non-stop learning.\\n \\\n",
    "              3\\t`` Sad '' wss not the right word , of course .\\n \\\n",
    "              5,14\tJust before Myra left -- Sue was saying good-by to Cathy , and she didm't realize I was near '' .\") as f:\n",
    "    for (locations, spellchk_sent) in spellchk(f):\n",
    "        print(\"{locs}\\t{sent}\".format(\n",
    "            locs=\",\".join([str(i) for i in locations]),\n",
    "            sent=\" \".join(spellchk_sent)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can see that \"was\" is detected correctly but \"Sue\" is replaced with \"susie\" and \"didm't\" is replaced with \"immediately\"! Because all the characters of \"Sue\" and \"didm't\" are available in \"susie\" and \"immediately\". Therefore, set intersection might not be a good choice.\n",
    "\n",
    "That's why we switched to a better distance metric between two strings which measures the number of single-character edits needed to change one word to another. It is called \"Levenshtein\" distance. In this case, the accuracy became 0.52."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_correction(typo, predict):\n",
    "    # return the most likely prediction for the mask token\n",
    "    edit_dist = [Levenshtein.distance(typo.lower(), predict[i]['token_str']) for i in range(len(predict))]\n",
    "    best_ind = np.argmin(edit_dist)\n",
    "    return predict[best_ind]['token_str']\n",
    "\n",
    "def spellchk(fh):\n",
    "    for (locations, sent) in get_typo_locations(fh):\n",
    "        spellchk_sent = sent\n",
    "        for i in locations:\n",
    "            # predict top_k replacements only for the typo word at index i\n",
    "            predict = fill_mask(\n",
    "                \" \".join([ sent[j] if j != i else mask for j in range(len(sent)) ]), \n",
    "                top_k=20\n",
    "            )\n",
    "            logging.info(predict)\n",
    "            spellchk_sent[i] = select_correction(sent[i], predict)\n",
    "        yield(locations, spellchk_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\tit will put your mind into non-stop learning.\n",
      "3\t`` Sad '' was not the right word , of course .\n",
      "5,14\tJust before Myra left -- she was saying good-by to Cathy , and she did realize I was near '' .\n"
     ]
    }
   ],
   "source": [
    "with StringIO(\"4\\tit will put your maind into non-stop learning.\\n \\\n",
    "              3\\t`` Sad '' wss not the right word , of course .\\n \\\n",
    "              5,14\tJust before Myra left -- Sue was saying good-by to Cathy , and she didm't realize I was near '' .\") as f:\n",
    "    for (locations, spellchk_sent) in spellchk(f):\n",
    "        print(\"{locs}\\t{sent}\".format(\n",
    "            locs=\",\".join([str(i) for i in locations]),\n",
    "            sent=\" \".join(spellchk_sent)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, \"Sue\" is correctly replaced with \"she\", but the problem is that \"she\" is not capital. The next change we made, was to capitalize the output words if the typo is capital. The other thing we did was to convert the Levenshtein distance to a probability-like number and combine it with the language model score. The latter didn't improve the accuracy. However, capitalization improved it to 0.57."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_correction(typo, predict):\n",
    "    # return the most likely prediction for the mask token\n",
    "    LM_score = np.array([predict[i]['score'] for i in range(len(predict))])\n",
    "    edit_dist = np.array([Levenshtein.distance(typo.lower(), predict[i]['token_str']) for i in range(len(predict))])\n",
    "    edit_score = (edit_dist.max() - edit_dist) / edit_dist.max()\n",
    "    w = 0.95\n",
    "    score_tot = w * edit_score + (1-w) * LM_score\n",
    "    best_ind = np.argmax(score_tot)\n",
    "    return predict[best_ind]['token_str'].capitalize() if typo[0].isupper() else predict[best_ind]['token_str']\n",
    "def spellchk(fh):\n",
    "    for (locations, sent) in get_typo_locations(fh):\n",
    "        spellchk_sent = sent\n",
    "        for i in locations:\n",
    "            # predict top_k replacements only for the typo word at index i\n",
    "            predict = fill_mask(\n",
    "                \" \".join([ sent[j] if j != i else mask for j in range(len(sent)) ]), \n",
    "                top_k=20\n",
    "            )\n",
    "            logging.info(predict)\n",
    "            spellchk_sent[i] = select_correction(sent[i], predict)\n",
    "        yield(locations, spellchk_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\tit will put your mind into non-stop learning.\n",
      "3\t`` Sad '' was not the right word , of course .\n",
      "5,14\tJust before Myra left -- She was saying good-by to Cathy , and she did realize I was near '' .\n",
      "0\tNormally the farm should be on an all-weather road .\n"
     ]
    }
   ],
   "source": [
    "with StringIO(\"4\\tit will put your maind into non-stop learning.\\n \\\n",
    "              3\\t`` Sad '' wss not the right word , of course .\\n \\\n",
    "              5,14\tJust before Myra left -- Sue was saying good-by to Cathy , and she didm't realize I was near '' .\\n \\\n",
    "              0\tOOviously the farm should be on an all-weather road .\") as f:\n",
    "    for (locations, spellchk_sent) in spellchk(f):\n",
    "        print(\"{locs}\\t{sent}\".format(\n",
    "            locs=\",\".join([str(i) for i in locations]),\n",
    "            sent=\" \".join(spellchk_sent)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, \"Sue\" is replaced with \"She\", which is completely correct. \n",
    "After checking different test cases, we noticed that the low accuracy of the approach stems from the language model. In other words, the correct word does not appear in the candidate list. For instance, in the above example, \"Obviously\" was not among the first 20 candidates. So, we thought maybe the language model is doing its job and we might just need to increase the number of candidates to capture the correct word. Therefore, we increased the number of candidates to 1000, which improved the accuracy significantly and made it 0.76."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_correction(typo, predict):\n",
    "    # return the most likely prediction for the mask token\n",
    "    LM_score = np.array([predict[i]['score'] for i in range(len(predict))])\n",
    "    edit_dist = np.array([Levenshtein.distance(typo.lower(), predict[i]['token_str']) for i in range(len(predict))])\n",
    "    edit_score = (edit_dist.max() - edit_dist) / edit_dist.max()\n",
    "    w = 0.95\n",
    "    score_tot = w * edit_score + (1-w) * LM_score\n",
    "    best_ind = np.argmax(score_tot)\n",
    "    return predict[best_ind]['token_str'].capitalize() if typo[0].isupper() else predict[best_ind]['token_str']\n",
    "def spellchk(fh):\n",
    "    for (locations, sent) in get_typo_locations(fh):\n",
    "        spellchk_sent = sent\n",
    "        for i in locations:\n",
    "            # predict top_k replacements only for the typo word at index i\n",
    "            predict = fill_mask(\n",
    "                \" \".join([ sent[j] if j != i else mask for j in range(len(sent)) ]), \n",
    "                top_k=1000\n",
    "            )\n",
    "            logging.info(predict)\n",
    "            spellchk_sent[i] = select_correction(sent[i], predict)\n",
    "        yield(locations, spellchk_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tObviously the farm should be on an all-weather road .\n"
     ]
    }
   ],
   "source": [
    "with StringIO(\"0\tOOviously the farm should be on an all-weather road .\") as f:\n",
    "    for (locations, spellchk_sent) in spellchk(f):\n",
    "        print(\"{locs}\\t{sent}\".format(\n",
    "            locs=\",\".join([str(i) for i in locations]),\n",
    "            sent=\" \".join(spellchk_sent)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, \"Obviously\" is also captured correctly. \n",
    "\n",
    "This was our final version of hw1 submitted to CourSys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group work\n",
    "\n",
    "* baa27 did all the work together with his groupmate mirzaei.\n",
    "* mirzaei did all the work together with his groupmate baa27.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
